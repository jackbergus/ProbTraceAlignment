\section{Modeling Probabilistic Dynamic Systems}
\label{sec:pns}
We recall the models and techniques at the basis for representing and computing probabilistic trace alignments.

\subsection{Stochastic Workflow Nets}\label{subsec:spn}
As customary in probabilistic conformance checking \cite{DBLP:conf/bpm/LeemansSA19,DBLP:conf/icpm/PolyvyanyyK19,DBLP:journals/tosem/PolyvyanyySWCM20}, we adopt stochastic Petri nets \cite{MarsanCB84,RoggeSoltiAW13} to represent processes. More specifically, we consider an interesting class of stochastic Petri nets with immediate (not timed) transitions only, namely untimed Stochastic Workflow Nets (\uswn{s}).
We fix a set $\alphabet = \tasks \cup \set{\tau}$ of labels, where labels in $\tasks$ indicate process tasks, whereas $\tau$ indicates an invisible execution step ($\tau$-transition). A \emph{trace} is a finite sequence of labels from $\tasks$.

\begin{definition} An \emph{untimed Stochastic Workflow Net (\uswn)}
is a tuple $\net = (P,T,F,\ell,W)$ where:
\begin{inparaenum}[\itshape (i)]
\item $(P,T,F)$ is a standard \emph{Workflow net} with places $P$, transitions $T$, and flow relation $F$ such that there is exactly one \emph{input place} with no incoming arc, and exactly one \emph{output place} with no outgoing arcs;
\item $\ell: T \rightarrow \alphabet$ is a \emph{labeling function} mapping each transition $t \in T$ into a label $\ell(t) \in \alphabet$ (if $\ell(t) = \tau$, then $t$ is a silent transition);
\item $W\colon T\to \mathbb{R}^+$ is a \emph{weight function} assigning a positive firing weight to each transition of the net.
\end{inparaenum}
\end{definition}
For an \uswn $\net$, we use dot notation to get its  components (e.g., $\net.P$ denotes its places). \emph{We do the same for the other structures introduced in the paper}. We use $\net.in$ and $\net.out$ to respectively denote the input and output places of $\net$.

As usual, the current state of execution for $\net$ is captured as a marking of the net: a multiset over places $\net.P$ indicating how many tokens populate each place.
%As pointed out above, \emph{we always assume, as customary in BPM, that the input \uswn is \underline{bounded}}, that is, in every state the number of tokens associated to each place cannot exceed a maximum, fixed threshold.
The notions of transition enablement and firing are also standard. Given a marking $\marking$ over \uswn $\net$, we denote by $\enaset{\marking}{\net}$ the set of enabled transitions in $\marking$; given transition $t \in \enaset{\marking}{\net}$, we write $\fire{\marking}{t}{\marking'}{\net}$ to capture that, within $\net$, firing $t$ in $\marking$ results in the new marking $\marking'$. A \emph{firing sequence of $\net$ starting from marking $\marking_0$} is a sequence $t_1\cdots t_n$ of transitions from $\net.T$ so that, for every $i \in \set{1,\ldots,n}$, we have that $\fire{\marking_{i-1}}{t_i}{\marking_{i}}{\net}$. We say that the firing sequence results in $\marking_{n}$.

As customary in Workflow nets, we consider two special markings: the \emph{input} (resp.~\emph{output}) marking $m_{in}^\net$ (resp.~$m_{out}^\net$) that assigns a single token to the input (resp.~output) place $\net.in$ (resp.~$\net.out$) of $\net$, and no token elsewhere. A \emph{valid sequence} $\seq = t_1\cdots t_n$ of $\net$ is a firing sequence of $\net$ starting from $m_{in}^\net$ and resulting in $m_{out}^\net$. A sequence $\run = \alpha_1 \cdots \alpha_n$ of labels from $\alphabet$ is a \emph{run} of $\net$ if there exists a valid underlying sequence $\seq = t_1\cdots t_n$ of $\net$  such that, for every $i \in \set{1,\ldots,n}$, we have $\net.\ell(t_i) = \alpha_i$. Run $\run$ may have different underlying valid sequences in $\net$, which we collectively refer to as $\seqs{\run}{\net}$. A trace $\nonlogtrace$ is a \emph{model trace} of $\net$ (or $\net$-trace for short) if there exists an underlying run $\run$ of $\net$ that corresponds to $\nonlogtrace$ once all occurrences of $\tau$ are removed. There may be multiple runs underlying an $\net$-trace $\nonlogtrace$, and we collectively refer to them as $\runs{\nonlogtrace}{\net}$. Finally, we denote the (possibly infinite) set of $\net$-traces as $\traces{\net}$.
%\begin{example} %\small
%\label{ex:net}
%\figurename~\ref{fig:spn} shows an example of an \uswn with input place $p_1$ and output place $p_7$. One run of the net is $\const{\tau c \tau a a \tau}$, which corresponds to trace $\const{caa}$. Overall, the net supports infinitely many finite traces of the form (represented using regular expressions):
%\begin{inparaenum}[\it (i)]
%\item $\const{aa^*}$,
%\item $\const{cb}$,
%\item $\const{caa^*}$.
%\end{inparaenum}
%\end{example}

For an \uswn, the key change to the standard execution semantics is that, being the net stochastic, the set of enabled transitions in a marking is associated to a discrete probability distribution. This is defined using the weight function of $N$: given marking $\marking$ of $N$ and an enabled transition $t \in \enaset{\marking}{\net}$, the \emph{firing probability} of $t$ in $\marking$ is $\probt{t}{\marking}{\net} = \frac{\net.W(t)}{\sum_{t'\in \enaset{\marking}{\net}}\net.W(t')}$. %It is easy to see that this is a probability distribution. %As required, the probabilities associated to all transitions enabled in a marking always add up to 1.
%Using firing probabilities as a basis, 
We use this to define the probability $\prob{\seq}{\net}$ of a valid sequence $\seq = t_1\cdots t_n$ of $\net$ as the product of the probabilities associated to each transition: $\prob{\seq}{\net} = \prod_{i \in \set{1,\ldots,n}}\probt{t_i}{\marking_{i-1}}{\net}$. %For a run $\run$ of $\net$, its probability $\prob{\seq}{\net}$ is then obtained by summing up the probabilities of all valid sequences corresponding to $\run$: $\prob{\run}{\net} = \sum_{\seq \in \seqs{\run}{\net}} \prob{\seq}{\net}$. Likewise, for a trace $\trace$ of $\net$, its probability is obtained by summing up
For a trace $\nonlogtrace$ of $\net$, its probability $\prob{\nonlogtrace}{\net}$ is then obtained by collecting all its underlying runs, in turn collecting all their underlying valid sequences, and summing up their respective probabilities: $\prob{\nonlogtrace}{\net} = \sum_{\run \in \runs{\nonlogtrace}{\net}} \sum_{\seq \in \seqs{\run}{\net}} \prob{\seq}{\net}$. This captures that, to observe $\nonlogtrace$, one can equivalently pick any of its underlying valid sequences. Notably, if a trace is not an $\net$-trace (i.e., it does not conform with $\net$), then its probability is 0. For convenience, when needed, we represent an $\net$-trace as a pair $\tup{\nonlogtrace,\prob{\nonlogtrace}{\net}}$.
%, where the probability assigned to $\nonlogtrace$ by $\net$ is retained.


\begin{remark}
\label{rem:collective}
The sum of the probabilities of all runs of an SWN, up to a certain maximum length $n$, is between 0 and 1. When $n$ tends to $\infty$, this sum tends to $1$. This is a direct consequence of how transition probabilities are computed, paired with the fact that runs of a Workflow net are maximal.
\end{remark}

By interpreting concurrency by interleaving, firing sequences and their probabilities are compactly represented in a reachability graph.


\begin{definition}
The \emph{Reachability Graph} $\rg{\net}$ of \uswn \net is a triple $(M,E,P)$ where:
\begin{inparaenum}[\itshape (i)]
\item $M$ is the set of reachable markings from $\marking_0^\net$ ($\marking_0^\net$ included);\item $E \subseteq M \times \alphabet \times M$ is a $\alphabet$-\emph{labeled transition relation} induced by $\net$, that is, for $\marking,\marking' \in M$, we have edge $(\marking,a,\marking') \in E$ if and only if there exists transition $t$ in $\net$ with label $\ell(t) = a$ and such that $\fire{\marking}{t}{\marking'}{\net}$;
\item $P:E \rightarrow [0,1]$ is the \emph{transition probability} function assigning to each transition $(\marking,a,\marking') \in E$ its  probability, obtained from the firing probability of the \uswn transition(s) that lead from $\marking$ to $\marking'$ and are labeled by $a$: $P(\marking,a,\marking') = \sum_{t_i \in \enaset{\marking}{\net} \text{ s.t.~} \net.\ell(t) = a \text{ and } \fire{\marking}{t}{\marking'}{\net}} \probt{t}{\marking}{\net}$.
\end{inparaenum}
\end{definition}
The definition also handles the special case where, in a given marking, distinct net transitions with the same label produce the same consequent marking: they are indistinguishable when observing the execution traces of the net, hence they collapse into a single edge of the reachability graph, where all firing probabilities are aggregated into a single value.
\figurename~\ref{fig:rg} shows an example of a reachability graph.



Every \uswn $\net$ handled in our framework is assumed to satisfy two natural properties:
\begin{compactenum}
\item $\net$ is \emph{bounded}, that is, every marking in $\rg{\net}$ assigns at most a pre-defined number of tokens to each place;
\item no loop in $\rg{\net}$ has all edges labeled by $\tau$.
\end{compactenum}
Property 1) states that process instances of $\net$ do not generate unboundedly many concurrent threads, i.e., that $\rg{\net}$ has finitely many states. Property 2) naturally corresponds to how $\tau$-transitions are used when modeling processes: they are essential to model  gateways (such as exclusive and parallel splits/joins), cascaded gateways without tasks in between, and skippable tasks; such constructs require  $\tau$-transitions, but never used in fully invisible loops.\footnote{A more thorough discussion on this can be found in \cite{Bergami21}.} 
%
Property 2) implies a very interesting property: given a trace $\nonlogtrace$, there are only boundedly many valid sequences that can produce it. Hence, the probability of $\nonlogtrace$ can be computed by:
\begin{inparaenum}[\it (i)]
\item exhaustively enumerating all its valid sequences;
\item calculating the probability of each such sequence;
\item summing up all the so-obtained probabilities.
\end{inparaenum}


%\begin{example} %\small
 % \label{ex:trace}
%Consider the \uswn \net of Example~\ref{ex:net}. Considering trace $\const{caa}$, it is easy to see that it has only one underlying run, namely $\const{\tau c \tau a a \tau}$, in turn produced by a single underlying valid sequence, and that
%The firing probability of picking the first $\tau$-transition starting from the input marking is $1$, as there are no alternatives. In the new marking, where only one token is assigned to $p_2$, the firing probability of choosing the $\tau$-transition above is $\rho_{23} = \frac{v_{\tau_2}}{v_{\tau_2}+v_c}$, whereas that of choosing the $c$-transition below is $\rho_{24} = \frac{v_c}{v_{\tau_2}+v_c}$. Upon choosing the transition below, the new marking assigns only to $p_4$ one token, leaving just one choice to continue by moving that token to $p_6$. In that marking, the probability of choosing the $a$-transition above is $\rho_{65} = \frac{v_{a_3}}{v_{a_3}+v_b}$, resulting in the token being moved to $p_5$. In this new marking, the probability of iterating over the $a$-transition above is $\rho_{55} = \frac{v_{\tau_3}}{v_{\tau_3}+v_{a_2}}$, while that of completing in the output marking via the enabled $\tau$-transition is $\rho_{57} = \frac{v_{\tau_3}}{v_{\tau_3}+v_{a_2}}$. Hence, all in all
%$\prob{\const{caa}}{\net} = 1 \cdot \rho_{24} \cdot 1 \cdot \rho_{65} \cdot \rho_{55} \cdot \rho_{57}$.
%\end{example}

\begin{remark}[From \cite{Bergami21}]
\label{rem:silence}
	For an SWN $\net$ with at most $b$ consecutive $\tau$ transitions, there are boundedly many runs of $\net$ yielding a given $\net$-trace $\nonlogtrace$: $\seqs{\nonlogtrace}{\net}$ contains runs whose maximum length is bounded by the length of $\nonlogtrace$ and $b$.
\end{remark}

By combining Remarks~\ref{rem:collective} and \ref{rem:silence}, we get a direct way of computing the trace probability $\prob{\nonlogtrace}{\net}$.
To handle probabilistic trace alignment, we need to relate an arbitrary log trace $\logtrace$ over $\tasks^*$ with the closest $\net$-traces that balance their distance from $\logtrace$ and their probability. Fortunately:

\begin{remark}
By increasing the length of the $\net$-traces, we reach a point where their probability and distance with respect to any log trace $\trace$ \emph{both} decrease. Intuitively, this is because executing too many loop iterations within $\net$ at once decreases the overall run probability and increments the distance from $\trace$.
\end{remark}

This implies that probabilistic trace alignment operates over a finite space of traces/runs, hence being a combinatorial problem that can be tackled with techniques such as $k$NN.



%Our working assumptions are the following:
%\begin{mylist}
%\item without any loss of generality for concurrent processes, we are interested in 1-bounded SPN, thus allowing an easy conversion from BPMN with no swim-lanes into 1-bounded SPN \cite{RaedtsPUWGS07}.
%\item As customary in trace alignment literature, two traces are equivalent iff. they are identical after stripping $\tau$ events.
%\item As we exploit SPN for generating the traces describing the business process via unfolding, we could discard SPNs containing either \cite{Bergami21}.
%\item Consequently, we could further restrict our analysis to SPNs containing silent transitions representing either skip-able sequences of non-silent transition, or the possibility of choosing different sequences of non-silent transitions \cite{Bergami21}.
%\end{mylist} We refer to a SPN satisfying such constraints as \textsc{Stochastic Workflow Nets} (SWN).




%\todo[inline]{Non capisco la frase}
%Given a log trace $\logtrace$ coming from a log file $\mathcal{L}$, we always associate to it a certain probability, i.e., $\prob{\logtrace}{\mathcal{L}}=1$.





%\begin{definition} An \emph{untimed Stochastic Workflow Net (\uswn)}
%	is a tuple $\net = (P,T,F,\ell,W)$ where:
%	\begin{inparaenum}[\itshape (i)]
%		%\begin{inparaenum}
%		\item $(P,T,F)$ is a standard \emph{Workflow net} with places $P$, transitions $T$, and flow relation $F$ such that there is exactly one \emph{input place} with no incoming arc, and exactly one \emph{output place} with no outgoing arcs;
%		\item $\ell: T \rightarrow \alphabet$ is a \emph{labeling function} mapping each transition $t \in T$ into a label $\ell(t) \in \alphabet$ - this either indicates the task executed upon firing $t$, or the fact that $t$ is an invisible transition (in the latter case, $\ell(t) = \tau$);
%		\item $W\colon T\to \mathbb{R}^+$ is a \emph{weight function} assigning a positive firing weight to each transition of the net.
%	\end{inparaenum}
%\end{definition}
%Given an \uswn $\net$, we use dot notation to extract its constitutive components (e.g., $\net.P$ denotes its places). \emph{The same dot notation will be used for the other structures introduced in the paper}. We also use $\net.in$ and $\net.out$ to respectively denote the input and output place of $\net$.
%The current state of execution is captured by a marking, i.e., a multiset of places $P$ indicating how many tokens populate each place.
%As pointed out above, \emph{we always assume, as customary in BPM, that the input \uswn is \underline{bounded}}, that is, in every state the number of tokens associated to each place cannot exceed a maximum, fixed threshold.
%The notions of transition enablement and firing are also the standard ones  \cite{MarsanCB84}, as well as the ones for \textit{firing probability} \cite{spdwe}. %: %, which provides the basis for capturing the stochastic behavior of the net.
%We use the following notation: given a marking $\marking$ over \uswn $\net$,  $\enaset{\marking}{\net}$ is the set of enabled transitions in $\marking$; given transition $t \in \enaset{\marking}{\net}$, we write $\fire{\marking}{t}{\marking'}{\net}$ to capture the fact that, with, firing $t$ in $\marking$ results in the new marking $\marking'$. A \emph{firing sequence starting from marking $\marking_0$} is a sequence $t_1\cdots t_n$ of transitions from $\net.T$ so that, for every $i \in \set{1,\ldots,n}$, we have that $\fire{\marking_{i-1}}{t_i}{\marking_{i}}{\net}$. We say that the firing sequence results in $\marking_{n}$.
%
%given a marking $\marking$ %of $N$
%and an enabled transition $t \in T_e$, the \emph{firing probability} of $t$ in $\marking$ is $\probt{t}{\marking}{\net} = \frac{W(t)}{\sum_{t'\in T_e}W(t')}$. %As required,
\begin{figure*}[t!]
	\hspace*{-1cm}\includegraphics[width=1.\textwidth]{images/pipeline}
	\caption{Proposed pipeline to assess the probabilistic trace alignment.}\label{fig:pipe}
\end{figure*}

%A \emph{valid sequence} $\seq = t_1\cdots t_n$ is a firing sequence starting from $m_{in}$ and resulting in $m_{out}$. The probability $\prob{\seq}{\net}$ of a valid sequence is the product of the probabilities associated to each transition. %: $\prob{\seq}{\net} = \prod_{i \in \set{1,\ldots,n}}\prob{t_i}{\marking_{i-1},\net}$. % A sequence of labels $\run = \alpha_1 \cdots \alpha_n$ from $\alphabet$ is a \emph{run} if there exists a valid underlying sequence $\seq = t_1\cdots t_n$  having $\alpha_i$ as a label for each $t_i\in \seq$. Run $\run$ may have different underlying valid sequences in $\seqs{\run}{\net}$.

%underlying run $\run$ corresponding to $\trace$ once all $\tau$ are removed.

\subsection{Transition Graphs}\label{subsec:ppn}

The graph and trace embedding techniques at the core of our approach cannot be directly defined over reachability graphs: they rely on graphs where edges are decorated by probabilities, and where labels are attached to nodes. In addition, to enable efficient algorithmic techniques, such graphs are compactly defined via transition matrixes. We hence take inspiration from \cite{GartnerFW03} and introduce \emph{probabilistic transition graphs}, used later to encode \uswn{s} via their reachability graphs.

%For a matrix $Q$ with row set $A$ and column set $B$, notation $[Q]_{ab}$ for $a \in A$ and $b \in B$ denotes the corresponding element in the matrix. In addition $\transp{Q}$ denotes the transposed matrix where rows and columns are inverted. We employ the usual sum and product operations over matrixes and arrays, and denote, for a square matrix $Q$, the repeated multiplication of $Q$ with itself $n$ times by $Q^n$.\todo{Rimuovere questo paragrafo se serve spazio,}\todo{NOn si capisce il significato di $\omega$}
%In our technical treatment, we continue to assume the existence of a set $\alphabet$ of labels (including the special label $\tau$).
\begin{definition} A \emph{(Probabilistic) Transition Graph} is a tuple $(V,s,t,L,R)$ where:
  \begin{inparaenum}[\itshape (i)]
    \item $V \subset \mathbb{N}$ is a set of \emph{nodes};
    \item $s\in V$ is the \emph{initial node};
    \item $e\in V$ is the \emph{accepting node};
    \item $L: \alphabet \times V \rightarrow \{0,1\}$ is a \emph{label matrix} associating each node in $V$ to a single label in $\alphabet$, where for label $\alpha \in \alphabet$ and node $\ind{i} \in V$, $[L]_{\alpha\ind{i}}$ gives $1$ if $\ind{i}$ is labeled by $\alpha$, $0$ otherwise;
    \item $R: V \times V \rightarrow [0,1]$ is a \emph{(probabilistic) transition matrix} indicating, for each pair of nodes, the probability of executing a transition from the first node leads to the second node.
   % \item $\omega \in [0,1]$ is a \emph{graph weight} indicating an overall value associated to the entire graph.
  \end{inparaenum}
$L$ and $R$ satisfy the following well-formedness conditions:
\begin{inparaenum}[\itshape (i)]
\item for every $i \in V$ there is one and only one label $\alpha \in \alphabet$ so   that $[L]_{\alpha\ind{i}}=1$;
\item  for  every $\ind{i} \in V$, $\sum_{\ind{j}\in V}[R]_{\ind{ij}}=1$.
\end{inparaenum}
\end{definition}
The condition for $L$ indicates that each node is mapped by $L$ to a single label, while the same label may be used for multiple nodes. The condition for $R$ ensures that the values contained therein can be interpreted as a probability distribution when choosing which next node to pick upon executing a transition. Matrices $L$ and $R$ can be exploited to determine the probability of reaching a node labeled by $\beta\in\Sigma$ from any node labeled $\alpha\in\Sigma$ in $n$ steps with $[LR^n\transp{L}]_{\alpha\beta}/[L\transp{L}]_{\alpha\alpha}$ that we shorthand as $[G.\Lambda^n]_{\alpha\beta}$\cite{GartnerFW03}.

A transition graph $\tg$ can be visualized as shown in \figurename~\ref{fig:orig} (\figurename~\ref{fig:closed} after $\tau$-closure, see below). There, the elements have the obvious interpretation, with the only important consideration that an edge from node $\ind{i}$ to node $\ind{j}$ is only depicted if the transition probability $[\tg.R]_{\ind{i}\ind{j}}$ is positive.
%There, each node $\ind{i} \in \tg.V$ is  represented as a circle with its identifying number. The initial node is decorated by a small incoming edge, while the final node is double circled. The label of the node is shown close to the circle, in agreement with $\tg.L$. Finally, an edge from $\ind{i} \in \tg.V$ to $\ind{j} \in \tg.V$ is shown if the transition probability $[\tg.R]_{\ind{i}\ind{j}}$ is positive. Each edge is decorated with the positive probability assigned by $\tg.R$.

%\begin{definition}[Path, trace]
%A \emph{path} in a transition graph $\tg$ is a finite sequence of nodes $\ind{i}_1 \cdots \ind{i}_n$ (with $n > 1$) such that, for every $j \in \set{1,\ldots,n-1}$, we have that $[\tg.R]_{\ind{i}_j\ind{i}_{j+1}} > 0$. Such a path is \emph{valid} if it starts from the initial node and ends in the accepting node of $\tg$, that is, $\ind{i}_1 = \tg.s$ and $\ind{i}_n = \tg.e$.
%
%A \emph{trace} is a finite sequence of nodes that can be turned into a valid sequence by introducing in the sequence an arbitrary number of $\tau$ labels (so as to account for hidden transitions in the graph).
%\end{definition}
%From the definition, it is clear that every valid path can be straightforwardly converted into a corresponding trace by removing all $\tau$ labels from the sequence.

%$\npath{\ind{i}}{\ind{j}}$

We mirror the definitions of \uswn{s} considering that now labels are on nodes. A \emph{valid sequence} of $\tg$ is a sequence $\ind{i}_0\ldots\ind{i}_n$ of nodes in $\tg.V$ from the initial to the accepting node that only traverses transitions with nonzero probability:
\begin{inparaenum}[\it (i)]
\item $\ind{i}_0 = \tg.s$;
\item $\ind{i}_n = \tg.e$;
\item if the sequence contains at least two nodes, each two consecutive nodes are connected by a positive transition probability, i.e., for every $j \in \set{1,\ldots,n}$ we have $[R]_{\ind{i}_{j-1}\ind{i}_{j}} > 0$.
\end{inparaenum}
Runs and model traces of transition graphs are then defined as in \uswn{s}, and we employ the same notation to indicate the runs underlying a model trace, and the valid sequences underlying a run. The computation of probabilities for runs and traces is hence defined equivalently.

%We close this section by introducing how some  matrix operations defined in the literature \cite{GartnerFW03} are applied to matrixes $L$ and $R$ of $\tg$, towards tackling interesting probability computations. These will be instrumental later on in the paper. Given two nodes $\ind{i},\ind{j} \in \tg.V$, $[R^n]_{\ind{i}\ind{j}}$ returns the probability of having a path in $\tg$ that connects $\ind{i}$ to $\ind{j}$ and has length $n$. Given two labels $\alpha,\beta \in \alphabet$, with $[LR^n\transp{L}]_{\alpha\beta}/[L\transp{L}]_{\alpha\alpha}$, we obtain the probability that, starting in any node labeled by $\alpha$, we reach a node labeled by $\beta$ through  $n$ consecutive steps in $\tg$. As a shortcut notation, we call the result $[\tg.\Lambda^n]_{\alpha\beta}$. Since there may be different nodes labeled by $\alpha$, we need to normalize the resulting probabilities. This is obtained with the division by $L\transp{L}$, which does so by assuming a uniform distribution when picking from which specific $\alpha$-labeled node one wants to start. Notice that these calculations need to be refined so as to consider proper runs and  traces. This will be done in Sect. ~\ref{subsec:as}. %\todo{Rimandare alla sezione giusta}



%
%$\texttt{\color{blue}i}\overset{n}{\rightsquigarrow}\texttt{\color{blue}j}$ of length $n$: therefore, $[\Lambda^n]_{\color{green}\alpha\beta}:=[LR^nL^t]_{\color{green}\alpha\beta}/[LL^t]_{\color{green}\alpha\alpha}$ denotes the probability that, having started at any node labeled $\color{green}\alpha$ and taking $n$ steps, we arrive at any node labeled $\color{green}\beta$ (${\color{green}\alpha}\overset{n}{\rightsquigarrow}{\color{green}\beta}$). We denote as ${\color{green}\alpha}{\rightsquigarrow}{\color{green}\beta}$ an aforementioned path of arbitrary length.
%We can also associate a weight $\omega\in[0,1]\subseteq\mathbb{R}$ to a TG, so to express the probability associated with the TG itself as valid.
%




%\begin{example}
%We can graphically represent such TG as in \cite{Myers1989}.
%Figure \ref{fig:orig} is a  TG $P^*=(\mathtt{\color{blue}1},\mathtt{\color{blue}8},L,R,1)$ where $\omega=1$, where the matrices $L$ and $R$ can be both defined as follows:
%$$L:=\kbordermatrix{
%             & \texttt{\color{blue}1}&\texttt{\color{blue}2}&\texttt{\color{blue}3}&\texttt{\color{blue}4}&\texttt{\color{blue}5}&\texttt{\color{blue}6}&\texttt{\color{blue}7}&\texttt{\color{blue}8}&\texttt{\color{blue}9}&\texttt{\color{blue}10}\\
%\color{green}\varepsilon  & \textbf{1}&0&0&0&0&0&\textbf{1}&\textbf{1}&\textbf{1}&\textbf{1}\\
%\color{green}a            & 0&\textbf{1}&0&\textbf{1}&0&\textbf{1}&0&0&0&0\\
%\color{green}b            & 0&0&0&0&\textbf{1}&0&0&0&0&0\\
%\color{green}c            & 0&0&\textbf{1}&0&0&0&0&0&0&0\\
%}\qquad R:=\kbordermatrix{
%& \texttt{\color{blue}1}&\texttt{\color{blue}2}&\texttt{\color{blue}3}&\texttt{\color{blue}4}&\texttt{\color{blue}5}&\texttt{\color{blue}6}&\texttt{\color{blue}7}&\texttt{\color{blue}8}&\texttt{\color{blue}9}&\texttt{\color{blue}10}\\
%\texttt{\color{blue}1}  & 0&0&{\color{red}p_2}&0&0&0&0&0&{\color{red}p_1}&0\\
%\texttt{\color{blue}2}  & 0&0&0&0&0&{\color{red}p_3}&{\color{red}p_6}&0&0&0\\
%\texttt{\color{blue}3}  & 0&0&0&0&0&0&0&0&0&{\color{red}1}\\
%\texttt{\color{blue}4}  & 0&0&0&0&0&{\color{red}p_3}&{\color{red}p_6}&0&0&0\\
%\texttt{\color{blue}5}  & 0&0&0&0&0&0&0&{\color{red}1}&0&0\\
%\texttt{\color{blue}6}  & 0&0&0&0&0&{\color{red}p_3}&{\color{red}p_6}&0&0&0\\
%\texttt{\color{blue}5}  & 0&0&0&0&0&0&0&{\color{red}1}&0&0\\
%\texttt{\color{blue}8}  & 0&0&0&0&0&0&0&0&0&0\\
%\texttt{\color{blue}9}  & 0&{\color{red}1}&0&0&0&0&0&0&0&0\\
%\texttt{\color{blue}10}  & 0&0&0&{\color{red}p_4}&{\color{red}p_5}&0&0&0&0&0\\
%}$$
%\end{example}

% Given a TG $P=(s,t,L,R,\omega)$, a trace $\tau$ is a tuple in $(\Sigma\backslash\{\varepsilon\})^*$ denoting a path always originating from $s$ and terminating in $t$.




\subsection{Graph and String Kernels}\label{subsec:katk}
As a foundational basis to compute trace alignments, we adapt similarity measures from the database literature.  Given a set of data examples $\mathcal{X}$, (e.g., strings or traces, transition graphs) a (positive definite) \emph{kernel} function $k\colon \mathcal{X}\times \mathcal{X}\to \mathbb{R}$ denotes the similarity of elements in $\mathcal{X}$. If $\mathcal{X}$ is the $d$-dimensional Euclidean space $\mathbb{R}^d$, the simplest kernel function is the inner product $\Braket{\mathbf{x},\mathbf{x}'}=\sum_{1\leq i\leq d}\mathbf{x}_i\mathbf{x}'_i$.
A kernel is said to \emph{perform ideally} \cite{Gartner03} when $k(x,x')=1$ whenever $x$ and $x'$ are the same object (\textit{strong equality}) and $k(x,x')=0$ whenever $x$ and $x'$ are distinct objects (\textit{strong dissimilarity}). A kernel is also said to be \emph{appropriate} when similar elements $x,x'\in\mathcal{X}$ are also close in the feature space. Notice that appropriateness can only be assessed empirically \cite{Gartner03}.
A positive definite kernel induces a distance metric as
$
d_k(\mathbf{x},\mathbf{x}'):=\sqrt{k(\mathbf{x},\mathbf{x})-2k(\mathbf{x},\mathbf{x}')+k(\mathbf{x}',\mathbf{x}')}
$.
When the kernel of choice is the inner product, the resulting distance is the Euclidean distance $\norm{\mathbf{x}-\mathbf{x}'}{2}$. A normalized vector $\hat{\mathbf{x}}$ is defined as $\mathbf{x}/\norm{\mathbf{x}}{2}$. For a normalized vector, we can easily prove that: $\norm{\hat{\mathbf{x}}-\hat{\mathbf{x}}'}{2}^2=2(1-\Braket{\hat{\mathbf{x}},\hat{\mathbf{x}}'})$.
When $\mathcal{X}$ does not represent directly the $d$-dimensional Euclidean space $\mathbb{R}^d$, we can use an \emph{embedding} $\embed\colon\mathcal{X}\to \mathbb{R}^d$ to define a kernel $k_\embed\colon \mathcal{X}\times \mathcal{X}\to\mathbb{R}$ as $k_\embed(x,x'):=\Braket{\embed(x),\embed(x')}$.
%As a result, $k_\embed(x,x')=k_\embed(x',x)$ for each $x,x'\in\mathcal{X}$.



The literature also provides a kernel representation for strings \cite{GartnerFW03}, which we can directly employ for traces. Specifically, if we associate each dimension in $\mathbb{R}^d$ to a different sub-string $\alpha\beta$ of size $2$ (i.e., $2$-grams\footnote{\label{fn:caveat}For our experiments, we choose to consider only $2$-grams but any $p$-grams of arbitrary length $p\geq 2$ might be adopted \cite{Gartner03}. An increased size of $p$ improves precision but also incurs in a worse computational complexity, as it requires to consider all the arbitrary sub-traces of length $p$ whose constitutive elements occur at any distance from each other within the trace.}), the embedding represents how frequently and ``compactly'' this sub-trace is embedded in the trace $\trace$ of interest. Therefore, we introduce a \emph{decay factor} $\lambda\in[0,1]\subseteq\mathbb{R}$ that, for all $m$ sub-strings where $\alpha$ and $\beta$ appear in $\trace$ at the same relative distance $z < |\trace|$, weights the resulting embedding as $\lambda^zm$.



\begin{table*}[t!]
	\vspace{+0.5cm}
	\caption{Embedding of traces $\const{caba}$, $\const{caa}$ and $\const{cb}$.}\label{tb:embedding}
	\vspace{-0.4cm}
	\begin{center}
		%\scalebox{0.6}
		{
			\begin{tabularx}{\textwidth}{
					>{\hsize=.1\hsize}X
					>{\hsize=.2\hsize}X
					>{\hsize=.1\hsize}X
					>{\hsize=.1\hsize}X
					>{\hsize=.1\hsize}X
					>{\hsize=.1\hsize}X
					>{\hsize=.1\hsize}X
					>{\hsize=.25\hsize}X
					>{\hsize=.2\hsize}X
					>{\hsize=.1\hsize}X
				}
				\toprule
				& $\const{aa}$    & $\const{ab}$   & $\const{ac}$    & $\const{ba}$   & $\const{bb}$   & $\const{bc}$ & $\const{ca}$ & $\const{cb}$ & $\const{cc}$   \\
				\midrule
				$\const{caba}$ & $\lambda^2$ & $\lambda$ & $0$ & $\lambda$  & $0$  & $0$ & $\lambda+\lambda^3$ & $\lambda^2$ & $0$\\
				%$\const{caaa}$ & $2\lambda+\lambda^2$& $0$ & $0$ & $0$ & $0$ & $0$ & $\lambda+\lambda^2+\lambda^3$ & $0$ & $0$ \\
				$\const{caa}$  & $\lambda$ & $0$ & $0$ & $0$ & $0$ & $0$ & $\lambda+\lambda^2$ & $0$&  $0$\\
				$\const{cb}$   & $0$ & $0$ & $0$ & $0$ & $0$ & $0$ & $0$ & $\lambda$& $0$ \\
				\bottomrule
			\end{tabularx}
		}
		\vspace{-0.3cm}
	\end{center}
\end{table*}
\begin{example}\label{ex:wheredotiszero} %\small
	Consider tasks $\tasks=\Set{\const{a},\const{b},\const{c}}$. The 2-grams over $\tasks$ are $\tasks^2=\Set{\const{aa},\const{ab},\const{ac},\const{ba},\const{bb},\const{bc},\const{ca},\const{cb},\const{cc}}$. \tablename~\ref{tb:embedding} shows the embeddings of some traces. Being a 2-gram, trace $\const{cb}$ has only one nonzero component, corresponding to itself, with $\trembed_{\const{cb}}(\const{cb})=\lambda$. Trace $\const{caa}$ has the 2-gram $\const{ca}$ occurring with length $1$ ($\const{\underline{ca}a}$) and $2$ ($\const{\underline{c}a\underline{a}}$), and the 2-gram $\const{aa}$ occurring with length $1$ ($\const{c\underline{aa}}$). Hence: $\trembed_{\const{ca}}(\const{caa})=\lambda+\lambda^2$ and  $\trembed_{\const{aa}}(\const{caa})=\lambda$.  Similar considerations apply for the other traces.
	We now want to compute the similarity between trace $\const{caba}$ and the other two traces. To do so, we sum, column-wise (that is, 2-gram by 2-gram) the product of the embeddings for each pair of traces. We then get $k_{\trembed}(\const{caba},\const{caa})=\lambda^3+(\lambda+\lambda^3)(\lambda+\lambda^2)$ and $k_{\trembed}(\const{caba},\const{cb})=\lambda^3
	$,
	%{\footnotesize
	%\[
	%k_{\trembed}(\const{caba},\const{caaa})=\lambda(\lambda+\lambda^2+\lambda^3)
	%~~
	%k_{\trembed}(\const{caba},\const{caa})=\lambda(\lambda+\lambda^2)
	%~~
	%k_{\trembed}(\const{caba},\const{cb})=\lambda(\lambda+\lambda^3)
	%\]}
	which induces the ranking $
	k_{\trembed}(\const{caba},\const{caa})>
	k_{\trembed}(\const{caba},\const{cb})
	$.
\end{example}


This trace kernel returns strong dissimilarity when the two traces have no shared 2-grams at any arbitrary length, but does not enjoy strong equality: the similarity of a trace with itself is at least $\lambda^2$ (returned when the trace is a 2-gram).

%Nevertheless, such string embedding has several shortcomings: \begin{alphalist}
%	\item it is not possible to numerically assess if two equal embeddings represent the same trace or come out from different traces;
%	\item it does not characterize $\tau$-moves; and
%	\item it is affected by numerical errors from finite arithmetic: longer traces generated from skewed probability distributions
%	yield greater truncation errors, as smaller $\lambda^i$ components for $i$ sufficiently large are ignored, preventing a complete numerical vector characterization of the trace in practice.
%\end{alphalist}

\endinput




