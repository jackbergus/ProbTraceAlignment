\section{Foundational Components and Assumptions}
We start by introducing the foundational components of our approach, and the corresponding working assumptions. On the one hand, we describe the class of  Petri nets we consider to represent stochastic processes. On the other hand, we recall graph and string kernels, which we use to compute probabilistic alignments.

\subsection{Stochastic Workflow Nets}\label{subsec:spn}
%As customary in probabilistic conformance checking \cite{DBLP:conf/bpm/LeemansSA19,DBLP:conf/icpm/PolyvyanyyK19,DBLP:journals/tosem/PolyvyanyySWCM20}, we adopt stochastic Petri nets \cite{MarsanCB84,RoggeSoltiAW13} as the underlying formal basis to represent processes. More specifically, we consider an interesting class of stochastic Petri nets with only immediate transitions (i.e., no timed ones).
%We assume to have a set $\alphabet = \tasks \cup \set{\tau}$ of labels, where labels in $\tasks$ indicate process tasks, whereas $\tau$ indicates an invisible execution step ($\tau$-transition). A \emph{trace} is a finite sequence of labels from $\tasks$. An \emph{untimed Stochastic Workflow Net (\uswn)}
%is a tuple $\net = (P,T,F,\ell,W)$ where:
%\textit{(i)} $(P,T,F)$ is a standard \emph{Workflow net} with places $P$, transitions $T$, and flow relation $F$ such that there is exactly one \emph{input place} with no incoming arc, and exactly one \emph{output place} with no outgoing arcs;
%\textit{(ii)} $\ell: T \rightarrow \alphabet$ is a \emph{labeling function} mapping each transition $t \in T$ into a label $\ell(t) \in \alphabet$;
%\textit{(iii)} $W\colon T\to \mathbb{R}^+$ is a \emph{weight function} assigning a positive firing weight to each transition.
%
%\texttt{[TODO: describe the notion of marking]}
%
%By interpreting concurrency by interleaving, we can represent all transition firings of an \uswn, together with their probabilities, in a reachability graph  $(M,E,P)$ where
%\textit{(i)} $M$ is the set of all reachable markings from the initial markings,
%\textit{(ii)} $E \subseteq M \times \alphabet \times M$ is a $\alphabet$-\emph{labeled transition relation} induced by $\net$, that is, for $\marking,\marking' \in M$, we have edge $(\marking,a,\marking') \in E$ if and only if there exists transition $t$ in $\net$ with label $\ell(t) = a$ and such that $\fire{\marking}{t}{\marking'}{\net}$, and
%\textit{(iii)} $P:E \rightarrow [0,1]$ is the \emph{transition probability} function assigning to each transition $(\marking,a,\marking') \in E$ its corresponding probability, obtained from the firing probability of the \uswn transition(s) that lead from $\marking$ to $\marking'$ and are labeled by $a$.
To model stochastic processes, we isolate an interesting class of Stochastic Petri nets \cite{MarsanCB84,Desel1998,RoggeSoltiAW13}. From the structural point of view, we consider $k$-bounded workflow nets with silent transitions. We assume a set $\alphabet = \tasks \cup \set{\tau}$ of labels, where labels in $\tasks$ indicate process tasks, whereas $\tau$ stands for an invisible execution step ($\tau$-transition). %Labels are associated to transitions via a labelling function $\lambda$.
From the stochastic point of view, do not consider timed aspects, and only focus on the definition of a probability distribution over enabled transitions. We call these nets \emph{stochastic workflow nets} (SWN for short)
Formally, an SWN is a tuple $\net=(P,T,F,W,i,f)$ where:
\begin{mylist}
	\item $P$ is a set of \textit{places};
	% to which we can associate a finite number of indistinguishable tokens;
	\item $T$ is a set of \textit{transitions} $t\in T$, to which we associate a label $\lambda(t)\in\Sigma$;
	\item $F\subseteq (P\times T)\cup (T\times P)$ is the \emph{flow relation} linking places to transitions and transitions to places; % to which we associate a \textit{firing cost} $\omega\colon F\to\mathbb{N}$;
	\item $W\colon T\to \mathbb{R}$ defines a \textit{firing weight} associated to each transition;
	\item the \emph{initial place} $i\in P$ has no ingoing edges; %($\not\exists t\in T. (t,i)\in F$);
	\item the \emph{final place} $f\in P$ has no outgoing edges. %($\not\exists t\in T. (f,t)\in F$).
\end{mylist}
The notions of marking, transition enablement, transition firing, and reachable markings, are as usual.
%A \textit{marking} is an assignment of a given amount of indistinguishable tokens to places described by a vector $M\colon P\to \mathbb{N}$. We say that a given transition $t$ is \textit{enabled} if $M(p)\geq \omega(p,t)$ for each ingoing $p$ to $t$ ($(p,t)\in F$). If such transition is enabled, then it can \textit{fire} a token. The set of the \textit{enabling transitions} $E(M)$ for a given marking $M$ are all the $t$ reachable from $p$ ($(p,t)\in F$) with $M(p)\neq 0$ where $t$ is enabled. When $t$ can fire a token for a marking $M$, we can generate a novel marking $M'$ from $M$ by moving the tokens from the ingoing places towards the outgoing places as
%$\forall p\in P.\; M'(p)=M(p)-[\omega(p,t)]+[\omega(t,p)]$.

We denote by $E(M)$ the set of transitions of $\net$ that are enabled in $M$. Given an initial marking $M$ over $\net$,  the \textit{reachability graph} of $(\net,M)$ is a graph $(\mathcal{M},\mathcal{E})$ with nodes $\mathcal{M}$ and $T$-labelled edges $\mathcal{E}$, where nodes $\mathcal{M}$ are all the reachable markings from $M$ ($M$ included), and where there is an edge from $M$ to $M'$ labeled by $t$ iff $t \in E(M)$ and firing $t$ in $M$ produces $M'$ (this is indicated by $M\overset{t}{\to}M'$).

We consider the two special markings $M_i$ and $M_f$ of $\net$, respectively assigning one token to the initial place $i$ and the final place $f$, and no tokens elsewhere. We say that $\net$ is $k$-\emph{bounded} if each marking in the reachability graph of $(\net,M_i)$ does assigns at most $k$ tokens to each place of $\net$. \emph{As a first, working assumption, we concentrate on $1$-bounded (i.e., safe) nets} (but our technique seamlessly carries over $k$-bounded nets as well).

As usual in stochastic nets, given a marking $M$ we use $W$ to induce a probability distribution over $E(M)$. This is done by assigning to each edge $M\overset{t}{\to}M'$ a corresponding transition probability $\mathbb{P}\left(M\overset{t}{\to}M'\right)=\frac{W(t)}{\sum_{t'\in E(M)}W(t')}$ \cite{spdwe}. Notice that, by construction, the probabilities associated to all enabled transitions in a marking always add up to 1.

A \emph{run} $\seq$ over $\net$ is a finite sequence $t_1\cdots t_n$ of transitions leading from $M_i$ to $M_f$ in the reachability graph of $(\net,M_i)$. The probability $\prob{\seq}{\net}$ of $\seq$ is obtained as as the product of the probabilities associated to each transition via $W$.

\begin{remark}
\label{rem:collective}
The sum of the probabilities of all runs of a SWN, up to a certain maximum length $n$, is always between 0 and 1. When $n$ tends to $\infty$, this sum tends to $1$. This is a direct consequence of how transition probabilities are computed, paired with the fact that runs of a workflow net are maximal.
\end{remark}

%To each of such edges $M\overset{t}{\to}M'$, we associate a transition probability. We say that a SPN with initial marking $M$ is $k$-\textit{bounded} if each of the nodes $M$ in the reachability graph have $\forall p\in P.\; M(p)\leq k$.

We now turn to traces, defined as finite sequence of labels from $\tasks$ (where each element witnesses the execution of a visible activity). The main issue is that, due to $\tau$-transitions, traces do not directly map into runs. To distinguish model traces from all the possible traces $\tasks^*$, we then proceed as follows. Following \cite{DBLP:conf/edoc/AdriansyahDA11,LeoniM17},  we say that a trace $\nonlogtrace=\const{a}_1\cdots \const{a}_m$ is a \emph{model trace for $\net$} ($\net$-trace for short) if there exists a run $\seq = t_1\cdots t_n$ where the sequence $\lambda(t_1)\cdots \lambda(t_n)$ of labels of $\seq$ coincides with $\nonlogtrace$ once all the $\tau$-labels are stripped away.

There may be multiple, possibly infinitely many runs yielding the same $\net$-trace. Given a $\net$-trace $\nonlogtrace$, we denote by $\seqs{\nonlogtrace}{\net}$ the set of such runs. The probability $\prob{\nonlogtrace}{\net}$ of $\nonlogtrace$ is then obtained by summing up the probabilities of all runs from $\seqs{\nonlogtrace}{\net}$.
This corresponds to the intuition that, to observe $\nonlogtrace$, one can equivalently pick any of its underlying runs. Notably, if a trace is not a $\net$-trace (i.e., it does not conform with $\net$), then its probability is 0.

To make this probability amenable to computation, we apply a \emph{second working assumption, introduced in \cite{Bergami21}, namely that of bounded silence}. A SWN has \emph{bounded silence} if there exists a fixed bound $b$ limiting the maximum number of consequent $\tau$-transitions between two visible transitions of the run. In \cite{Bergami21}, it is argued that this assumption is reasonable when modeling business processes: it retains the possibility of capturing gateways and skippable tasks, while forbidding whole cycles entirely characterized by $\tau$ transitions, which would in turn generate infinitely many runs yielding the same trace.

\begin{remark}
\label{rem:silence}
	For a SWN $\net$ with silence bounded by $b$, given a $\net$-trace $\trace$ there are boundedly many runs of $\net$ yielding $\trace$, that is, $\seqs{\nonlogtrace}{\net}$ has a size bounded by $b$, containing runs whose maximum length is bounded by the length of $\trace$ and $b$ \cite{Bergami21}.
\end{remark}

By combining Remarks~\ref{rem:collective} and \ref{rem:silence}, we thus get a direct way of computing the trace probability $\prob{\nonlogtrace}{\net}$.

Since our goal is to handle probabilistic trace alignment, we need to relate a (possibly non-model) arbitrary trace $\logtrace$ over $\tasks^*$ with the most closely $\net$-traces that balance their distance from $\logtrace$ and their probability. In this respect, we notice the following.

\begin{remark}
By increasing the length of $\net$-traces, we reach a point where their probability and distance w.r.t.~the log trace $\trace$ of interest \emph{both} decrease. Intuitively, this is because executing too many loop iterations within $\net$ decreases the overall run probability, and increments the distance to $\trace$.
\end{remark}

Thanks to this final remark, we get that the problem of probabilistic trace alignment works in a finite space of traces and runs, and it is consequently a combinatorial problem that can be attacked with techniques such as $k$-nearest neighbors.



%Our working assumptions are the following:
%\begin{mylist}
%\item without any loss of generality for concurrent processes, we are interested in 1-bounded SPN, thus allowing an easy conversion from BPMN with no swim-lanes into 1-bounded SPN \cite{RaedtsPUWGS07}.
%\item As customary in trace alignment literature, two traces are equivalent iff. they are identical after stripping $\tau$ events.
%\item As we exploit SPN for generating the traces describing the business process via unfolding, we could discard SPNs containing either \cite{Bergami21}.
%\item Consequently, we could further restrict our analysis to SPNs containing silent transitions representing either skip-able sequences of non-silent transition, or the possibility of choosing different sequences of non-silent transitions \cite{Bergami21}.
%\end{mylist} We refer to a SPN satisfying such constraints as \textsc{Stochastic Workflow Nets} (SWN).




%\todo[inline]{Non capisco la frase}
%Given a log trace $\logtrace$ coming from a log file $\mathcal{L}$, we always associate to it a certain probability, i.e., $\prob{\logtrace}{\mathcal{L}}=1$.





%\begin{definition} An \emph{untimed Stochastic Workflow Net (\uswn)}
%	is a tuple $\net = (P,T,F,\ell,W)$ where:
%	\begin{inparaenum}[\itshape (i)]
%		%\begin{inparaenum}
%		\item $(P,T,F)$ is a standard \emph{Workflow net} with places $P$, transitions $T$, and flow relation $F$ such that there is exactly one \emph{input place} with no incoming arc, and exactly one \emph{output place} with no outgoing arcs;
%		\item $\ell: T \rightarrow \alphabet$ is a \emph{labeling function} mapping each transition $t \in T$ into a label $\ell(t) \in \alphabet$ - this either indicates the task executed upon firing $t$, or the fact that $t$ is an invisible transition (in the latter case, $\ell(t) = \tau$);
%		\item $W\colon T\to \mathbb{R}^+$ is a \emph{weight function} assigning a positive firing weight to each transition of the net.
%	\end{inparaenum}
%\end{definition}
%Given an \uswn $\net$, we use dot notation to extract its constitutive components (e.g., $\net.P$ denotes its places). \emph{The same dot notation will be used for the other structures introduced in the paper}. We also use $\net.in$ and $\net.out$ to respectively denote the input and output place of $\net$.
%The current state of execution is captured by a marking, i.e., a multiset of places $P$ indicating how many tokens populate each place.
%As pointed out above, \emph{we always assume, as customary in BPM, that the input \uswn is \underline{bounded}}, that is, in every state the number of tokens associated to each place cannot exceed a maximum, fixed threshold.
%The notions of transition enablement and firing are also the standard ones  \cite{MarsanCB84}, as well as the ones for \textit{firing probability} \cite{spdwe}. %: %, which provides the basis for capturing the stochastic behavior of the net.
%We use the following notation: given a marking $\marking$ over \uswn $\net$,  $\enaset{\marking}{\net}$ is the set of enabled transitions in $\marking$; given transition $t \in \enaset{\marking}{\net}$, we write $\fire{\marking}{t}{\marking'}{\net}$ to capture the fact that, with, firing $t$ in $\marking$ results in the new marking $\marking'$. A \emph{firing sequence starting from marking $\marking_0$} is a sequence $t_1\cdots t_n$ of transitions from $\net.T$ so that, for every $i \in \set{1,\ldots,n}$, we have that $\fire{\marking_{i-1}}{t_i}{\marking_{i}}{\net}$. We say that the firing sequence results in $\marking_{n}$.
%
%given a marking $\marking$ %of $N$
%and an enabled transition $t \in T_e$, the \emph{firing probability} of $t$ in $\marking$ is $\probt{t}{\marking}{\net} = \frac{W(t)}{\sum_{t'\in T_e}W(t')}$. %As required,


%A \emph{valid sequence} $\seq = t_1\cdots t_n$ is a firing sequence starting from $m_{in}$ and resulting in $m_{out}$. The probability $\prob{\seq}{\net}$ of a valid sequence is the product of the probabilities associated to each transition. %: $\prob{\seq}{\net} = \prod_{i \in \set{1,\ldots,n}}\prob{t_i}{\marking_{i-1},\net}$. % A sequence of labels $\run = \alpha_1 \cdots \alpha_n$ from $\alphabet$ is a \emph{run} if there exists a valid underlying sequence $\seq = t_1\cdots t_n$  having $\alpha_i$ as a label for each $t_i\in \seq$. Run $\run$ may have different underlying valid sequences in $\seqs{\run}{\net}$.

%underlying run $\run$ corresponding to $\trace$ once all $\tau$ are removed.

\subsection{Graph and String Kernels}\label{subsec:katk}
 As a foundational basis to compute trace alignments, we adapt similarity measures from the database literature.  Given a set of data examples $\mathcal{X}$, (e.g., strings or traces, transition graphs) a (positive definite) \emph{kernel} function $k\colon \mathcal{X}\times \mathcal{X}\to \mathbb{R}$ denotes the similarity of elements in $\mathcal{X}$. If $\mathcal{X}$ is the $d$-dimensional Euclidean Space $\mathbb{R}^d$, the simplest kernel function is the inner product $\Braket{\mathbf{x},\mathbf{x}'}=\sum_{1\leq i\leq d}\mathbf{x}_i\mathbf{x}'_i$.
A kernel is said to \emph{perform ideally} \cite{Gartner03} when $k(x,x')=1$ whenever $x$ and $x'$ are the same object (\textit{strong equality}) and $k(x,x')=0$ whenever $x$ and $x'$ are distinct objects (\textit{strong dissimilarity}). A kernel is also said to be \emph{appropriate} when similar elements $x,x'\in\mathcal{X}$ are also close in the feature space. Notice that appropriateness can be only assessed  empirically \cite{Gartner03}.
A positive definite kernel induces a distance metric as
$
d_k(\mathbf{x},\mathbf{x}'):=\sqrt{k(\mathbf{x},\mathbf{x})-2k(\mathbf{x},\mathbf{x}')+k(\mathbf{x}',\mathbf{x}')}
$.
When the kernel of choice is the inner product, the resulting distance is the Euclidean distance $\norm{\mathbf{x}-\mathbf{x}'}{2}$. A normalized vector $\hat{\mathbf{x}}$ is defined as $\mathbf{x}/\norm{\mathbf{x}}{2}$. For a normalized vector we can easily prove that: $\norm{\hat{\mathbf{x}}-\hat{\mathbf{x}}'}{2}^2=2(1-\Braket{\hat{\mathbf{x}},\hat{\mathbf{x}}'})$.
When $\mathcal{X}$ does not represent directly a $d$-dimensional Euclidean space, we can use an \emph{embedding} $\embed\colon\mathcal{X}\to \mathbb{R}^d$ to define a kernel $k_\embed\colon \mathcal{X}\times \mathcal{X}\to\mathbb{R}$ as $k_\embed(x,x'):=\Braket{\embed(x),\embed(x')}$. As a result, $k_\embed(x,x')=k_\embed(x',x)$ for each $x,x'\in\mathcal{X}$.



 The literature also provides a kernel representation for strings \cite{LodhiSSCW02,GartnerFW03}: if we associate each dimension in $\mathbb{R}^d$ to a different sub-string $\alpha\beta$ of size $2$ (i.e., $2$-grams\footnote{\label{fn:caveat}For our experiments, we choose to consider only $2$-grams, but any $p$-grams of arbitrary length $p\geq 2$ might be adopted \cite{Gartner03}. An increased size of $p$ improves precision but also incurs in a worse computational complexity, as it requires to consider all the arbitrary subtraces of length $p$ whose constitutive elements occur at any distance from each other within the trace.}), it should represent how frequently and ``compactly'' this subtrace is embedded in the trace $\trace$ of interest. Therefore, we introduce a \emph{decay factor} $\lambda\in[0,1]\subseteq\mathbb{R}$ that, for all $m$ sub-strings where $\alpha$ and $\beta$ appear in $\trace$ at the same relative distance $l < |\trace|$, weights the resulting embedding as $\lambda^lm$.



\begin{table*}[t!]
	\vspace{+0.5cm}
	\caption{Embedding of traces $\const{caba}$, $\const{caa}$ and $\const{cb}$.}\label{tb:embedding}
	\vspace{-0.4cm}
	\begin{center}
		%\scalebox{0.6}
		{
			\begin{tabularx}{\textwidth}{
					>{\hsize=.1\hsize}X
					>{\hsize=.2\hsize}X
					>{\hsize=.1\hsize}X
					>{\hsize=.1\hsize}X
					>{\hsize=.1\hsize}X
					>{\hsize=.1\hsize}X
					>{\hsize=.1\hsize}X
					>{\hsize=.25\hsize}X
					>{\hsize=.2\hsize}X
					>{\hsize=.1\hsize}X
				}
				\toprule
				& $\const{aa}$    & $\const{ab}$   & $\const{ac}$    & $\const{ba}$   & $\const{bb}$   & $\const{bc}$ & $\const{ca}$ & $\const{cb}$ & $\const{cc}$   \\
				\midrule
				$\const{caba}$ & $\lambda^2$ & $\lambda$ & $0$ & $\lambda$  & $0$  & $0$ & $\lambda+\lambda^3$ & $\lambda^2$ & $0$\\
				%$\const{caaa}$ & $2\lambda+\lambda^2$& $0$ & $0$ & $0$ & $0$ & $0$ & $\lambda+\lambda^2+\lambda^3$ & $0$ & $0$ \\
				$\const{caa}$  & $\lambda$ & $0$ & $0$ & $0$ & $0$ & $0$ & $\lambda+\lambda^2$ & $0$&  $0$\\
				$\const{cb}$   & $0$ & $0$ & $0$ & $0$ & $0$ & $0$ & $0$ & $\lambda$& $0$ \\
				\bottomrule
			\end{tabularx}
		}
		\vspace{-0.3cm}
	\end{center}
\end{table*}
\begin{example}\label{ex:wheredotiszero} %\small
	Consider tasks $\tasks=\Set{a,b,c}$. The possible 2-grams over $\tasks$ are $\tasks^2=\Set{\const{aa},\const{ab},\const{ac},\const{ba},\const{bb},\const{bc},\const{ca},\const{cb},\const{cc}}$. Table~\ref{tb:embedding} shows the embeddings of some traces. Being a 2-gram, trace $\const{cb}$ has only one nonzero component, namely that corresponding to itself, with $\trembed_{\const{cb}}(\const{cb})=\lambda$. Trace $\const{caa}$ has the 2-gram $\const{ca}$ occurring with length $1$ ($\const{\underline{ca}a}$) and $2$ ($\const{\underline{c}a\underline{a}}$), and the 2-gram $\const{aa}$ with occurring length $1$ ($\const{c\underline{aa}}$). Hence: $\trembed_{\const{ca}}(\const{caa})=\lambda+\lambda^2$ and  $\trembed_{\const{aa}}(\const{caa})=\lambda$.  Similar considerations can be carried out for the other traces in the table.
	We now want to compute the similarity between the first trace $\const{caba}$ and the other two traces. To do so, we sum, column by column (that is, 2-gram by 2-gram) the product of the embeddings for each pair of traces. We then get $k_{\trembed}(\const{caba},\const{caa})=\lambda^3+(\lambda+\lambda^3)(\lambda+\lambda^2)$ and $k_{\trembed}(\const{caba},\const{cb})=\lambda^3
	$,
	%{\footnotesize
	%\[
	%k_{\trembed}(\const{caba},\const{caaa})=\lambda(\lambda+\lambda^2+\lambda^3)
	%~~
	%k_{\trembed}(\const{caba},\const{caa})=\lambda(\lambda+\lambda^2)
	%~~
	%k_{\trembed}(\const{caba},\const{cb})=\lambda(\lambda+\lambda^3)
	%\]}
	which induces ranking $
	k_{\trembed}(\const{caba},\const{caa})>
	k_{\trembed}(\const{caba},\const{cb})
	$.
\end{example}

Nevertheless, such string embedding has several shortcomings: \begin{alphalist}
	\item it is not weakly-ideal, so we cannot numerically assess if two embeddings represent equivalent traces
	(Example \ref{ex:wheredotiszero});
	\item it does not characterize $\tau$-moves, so the probabilities of the initial and final $\tau$-moves are not preserved; and
	\item it is affected by numerical errors from finite arithmetic: longer traces $\nonlogtrace$ generated from skewed probability
	distributions %$G.\Lambda^i$
	yield greater truncation errors, as smaller $\lambda^i$ components for bigger
	$i<|\nonlogtrace|$ are ignored, preventing a complete numerical vector characterization of  $\nonlogtrace$ in practice.
\end{alphalist}

\endinput




