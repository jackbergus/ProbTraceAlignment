\section{Working Assumptions}\label{sec:was}
In this paper, we assume that the model against which we want to align the log traces is fixed as in previous papers exploiting database technologies for business process management techniques \cite{SchonigRCJM16}. Therefore, we always assume to know all the traces associated to a procedural model, as well as their associated probabilities. As we will show in the next section, this assumption will make the alignment process more efficient, as we can focus efficiently aligning multiple log traces. 

This section shows that, starting from common assumptions from the BPM community, we are able to transform reachability graphs into a node-labeled Markovian Process, where  $R$ is the associated transition matrix and $L$ is the node-labeling matrix. In fact, such matrices can determine the probability of reaching a node labeled $\beta\in\Sigma$ from any node labelled $\alpha\in\Sigma$ in $n$ steps as in string kernels:   $[\Lambda^n]_{\alpha\beta}:=[LR^nL^\top]_{\alpha\beta}/[LL^\top]_{\alpha\alpha}$ (see \cite{GartnerFW03} and Example \ref{ex:wheredotiszero}). 
%
Our working assumptions are the following: \begin{mylist}
\item without any loss of generality for concurrent processes, we are interested in 1-bounded SPN, thus allowing an easy conversion from BPMN with no swim-lanes into 1-bounded SPN \cite{RaedtsPUWGS07}.
\item As customary in trace alignment literature \cite{DBLP:conf/edoc/AdriansyahDA11,LeoniM17}, two traces are equivalent iff. they are identical after stripping $\tau$ events.
\item As we exploit SPN for generating the traces describing the business process via unfolding, we could discard SPNs containing either whole paths or cycles entirely characterized by silent $\tau$ transitions, as it is always possible to generate SPNs without those and containing the same set of traces \cite{Bergami21}.
\item Consequently, we could further restrict our analysis to SPNs containing silent transitions representing either skip-able sequences of non-silent transition, or the possibility of choosing different sequences of non-silent transitions \cite{Bergami21}.
\end{mylist} We refer to a SPN satisfying such constraints as \textsc{Stochastic Workflow Nets} (SWN). 

As a first result, the set of all the traces generated from the unfolding og SWN partition the sample space. This is the result of the two following considerations: \begin{mylist} 
\item Given the requirements over the initial and final markings, we will generate a reachability graph $(\mathcal{M},\mathcal{E})$ where the initial (final) marking has no ingoing (outgoing) edges;
\item also, by definition of the transition probability, the sum of the transition probabilities from a marking $M$ towards the adjacent $M'$ always adds up to $1$.
\end{mylist}

Finally, we can intuitively prove the goal asserted at the beginning of the current section: first, we need to shift labels from edges to nodes, and then we perform $\tau$-closures while preserving (if required) $\tau$-transitions for both start and final nodes; such operations preserve the traces' probabilities \cite{Bergami21}. Next, the transition probabilities are going to be stored in a matrix $R$ representing a stochastic process, while for each node $i$ there is only one label $\alpha$ such that $[L]_{i\alpha}=1$, and $[L]_{i\beta}=0$ if $\alpha\neq\beta$. Therefore, runs for Markovian Processes are then defined as for SWNs, and we employ the same notation to indicate the valid sequences underlying a model trace. The computation of probabilities for traces is hence defined equivalently. We denote these pair of matrices as \textsc{Transition Graph} $TG=(L,R)$.

\endinput
Petri Nets and Generalized Stochastic Petri Nets are well-established formalisms \cite{DBLP:journals/tosem/PolyvyanyySWCM20} for modelling processes \cite{RoggeSoltiAW13} represented in the Petri Net Markup Language, supported by our tool. Due to the lack of space, we refer to \cite{spdwe} for the usual notation over Petri Nets. We restrict our interest to an interesting class of $1$-\textit{bounded} stochastic Petri nets with no timed transitions, namely \textsc{untimed Stochastic Workflow Nets} denoted as $N$. We now sketch the properties of the SWN sketched in \cite{Bergami21}. We consider a single \emph{input} (\emph{output}) marking $m_{in}$ ($m_{out}$) assigning a single token to the input (output) place, and no token elsewhere [\dots].


[\dots] These structural properties makes them suitable for converting BPMN models with no swim-lanes as SWN, which firing weight could be estimated from their associated log files \cite{spdwe}. 
\begin{example} %\small
	\label{ex:net}
	\figurename~\ref{fig:spn} shows an example of an \uswn with input place $p_1$ and output place $p_7$. One run of the net is $\const{\tau c \tau a a \tau}$, which corresponds to trace $\const{caa}$. Overall, the net supports infinitely many finite traces of the form (represented using regular expressions):
	\begin{inparaenum}[\it (i)]
		\item $\const{aa^*}$,
		\item $\const{cb}$,
		\item $\const{caa^*}$.
	\end{inparaenum}
\end{example}
%When executing an \uswn, the crucial addition to the standard execution semantics of Workflow nets is that, being the net stochastic, in each marking the set of enabled transitions gets associated to a discrete probability distribution. This is defined as follows: 
%given a marking $\marking$ of $N$ and an enabled transition $t \in \enaset{\marking}{\net}$, the \emph{firing probability} of $t$ in $\marking$ is $\probt{t}{\marking}{\net} = \frac{\net.W(t)}{\sum_{t'\in \enaset{\marking}{\net}}\net.W(t')}$. As required, the probabilities associated to all enabled transitions in a marking always add up to 1.
% %For a run $\run$ of $\net$, its probability $\prob{\seq}{\net}$ is then obtained by summing up the probabilities of all valid sequences corresponding to $\run$: $\prob{\run}{\net} = \sum_{\seq \in \seqs{\run}{\net}} \prob{\seq}{\net}$. Likewise, for a trace $\trace$ of $\net$, its probability is obtained by summing up
% For convenience, when needed, we represent an $\net$-trace as a pair $\tup{\trace,\prob{\trace}{\net}}$, where the probability assigned to $\trace$ by $\net$ is retained.

We need to lift this approach so as to consider all occurrences of subtraces $\alpha\beta$ at every distance between $1$ and $|\trace|-1$. To do so, we proceed in two steps. First, we encode $\trace$ into a ``linear'' transition graph $\tg_\trace$ (\figurename~\ref{fig:taustar}) in the obvious way. %\todo{Tagliare dopo i due punti se necessario.} each node in $G_\sigma.V$  corresponds to an element of the trace labeled correspondingly, and the nodes representing two consecutive elements in the trace are connected with a transition probability of 1 (whereas in all the other cases, the probability is 0).
As a second step, we rely on the matrix operations to calculate a simplified version of the embedding defined in \cite{LodhiSSCW02} as $\trembed_{\alpha\beta}(\trace)=\sum_{1\leq i\leq |\trace|}\lambda^i[(\tg_{\trace}.\Lambda)^i]_{\alpha\beta}$. %\todo{No spazio per spiegare cosa succede...}
%This value can be seen as a reward.
The kernel between two traces corresponds to the sum of the products of such values calculated 2-gram by 2-gram for the two traces.
%, namely it is equal to the \emph{kernel convolution}. %\todo{L'ho provato a scrivere intuitivamente, ma non e' chiaro da dove arrivi questo modo di calcolarlo... deriva dalle formule sopra ma la digressione in mezzo e' lunga. Come possiamo fare per chiarire? L'esempio spiega bene tutto!}
This trace kernel returns strong dissimilarity when the two traces have no shared 2-grams at any arbitrary occurring length, but does not enjoy strong equality (as the similarity of a trace with itself is at least $\lambda^2$ - returned when the trace is a 2-gram).

%
%we can represent it as a TG \cite{Myers1989} $(1,{|\tau|},L_\tau,R_\tau,1)$ having $[L_\tau]_{{\color{green}\alpha}\texttt{\color{blue}i}}=1\Leftrightarrow \tau_{\texttt{\color{blue}i}}={\color{green}\alpha}$ and $[L_\tau]_{{\color{green}\alpha}\texttt{\color{blue}i}}=0$ otherwise, and $\forall i<|\tau|.\; [R_\tau]_{\texttt{\color{blue}i(i+1)}}=1 $ and $[R_\tau]_{\texttt{\color{blue}ij}}=0$ otherwise.
%Exploiting this encoding, we can adopt a simplified version of the embedding defined in \cite{LodhiSSCW02,Raedt} as $\embed_{\mathcal{T}}(\tau)_{{\color{green}\alpha\beta}}=\sum_{1\leq i\leq |\tau|}\lambda^i[(\Lambda_\tau)^i]_{\color{green}\alpha\beta}$.
%Please note that this definition is similar to a transition matrix embedding proposed in \cite{GartnerFW03} via geometric series, that is $\sum_i\lambda^i[R^i]_{\color{green}\alpha\beta}$.

\begin{figure}[!t]
	\centering
	\includegraphics[width=.4\textwidth]{images/taustar.pdf}
	\caption{Graphical representation of the transition graph encoding trace $\const{caba}$.}\label{fig:taustar}
	
\end{figure}
%
%\begin{example}\label{ex:tracembed}
%	{Let us suppose that we want to align a trace $\tau^*$ to one of the traces from a transition graph: in order to carry out an approximate alignment, we need to transform it to a transition graph first.} A trace $\tau^*=\textup{caba}$ can be graphically represented in Figure \ref{fig:taustar}. The associated TG $T=(\mathtt{\color{blue}1},\mathtt{\color{blue}4},L,R,1)$ has matrices $L$ and $R$  defined as follows:
%	$$L:=\kbordermatrix{
%		& \texttt{\color{blue}1}&\texttt{\color{blue}2}&\texttt{\color{blue}3}&\texttt{\color{blue}4}\\
%		\color{green}a            & 0&\textbf{1}&0&\textbf{1}\\
%		\color{green}b            & 0&0&\textbf{1}&0\\
%		\color{green}c            & \textbf{1}&0&0&0\\
%	}\qquad R:=\kbordermatrix{
%		& \texttt{\color{blue}1}&\texttt{\color{blue}2}&\texttt{\color{blue}3}&\texttt{\color{blue}4}\\
%		\texttt{\color{blue}1}  & 0&\color{red}1&0&0\\
%		\texttt{\color{blue}2}  & 0&0&\color{red}1&0\\
%		\texttt{\color{blue}3}  & 0&0&0&\color{red}1\\
%		\texttt{\color{blue}4}  & 0& 0& 0& 0\\
%	}$$
%We can similarly represent all the traces from the USPN.
%\end{example}

%\begin{example}
%The subtrace \textit{\textbf{\uline{hi}}} is represented in \textit{\textbf{\uline{hi}}deous},   \textit{\uline{\textbf{h}}e\uline{{i}}d\textbf{i}}, and \textit{\uline{{\textbf{h}i}}nd\textbf{i}}, but with different frequencies and subtrace distances. We have $\embed_{\mathcal{T}}(\textit{hideous})_{{\color{green}hi}}=\lambda$,  $\embed_{\mathcal{T}}(\textit{heidi})_{{\color{green}hi}}=\lambda^2+\lambda^4$, and $\embed_{\mathcal{T}}(\textit{hindi})_{{\color{green}hi}}=\lambda+\lambda^4$.
%\end{example}

